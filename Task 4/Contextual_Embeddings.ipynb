{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8bd9b279",
      "metadata": {
        "scrolled": true,
        "id": "8bd9b279"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install nltk transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "091fd99b",
      "metadata": {
        "id": "091fd99b"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from transformers import TFAlbertModel, AlbertTokenizer, AlbertConfig\n",
        "import numpy as np\n",
        "import csv\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRQtcMT2aMOS",
        "outputId": "f99f0172-f8d3-499e-95c7-45016deef6ef"
      },
      "id": "WRQtcMT2aMOS",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b570fcde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b570fcde",
        "outputId": "f66aa64b-50a0-4841-a0a5-b86adc4a1cd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('brown')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3c0913b1",
      "metadata": {
        "id": "3c0913b1"
      },
      "outputs": [],
      "source": [
        "model_name = \"albert-base-v2\"\n",
        "\n",
        "original_config = AlbertConfig.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new configuration with reduced hidden size\n",
        "reduced_hidden_size = 12  # Replace with your desired hidden size\n",
        "reduced_num_hidden_layers = 1  # Choose a smaller number\n",
        "reduced_num_attention_heads = 2  # Choose a smaller number\n",
        "new_config = AlbertConfig(\n",
        "    vocab_size=original_config.vocab_size,\n",
        "    hidden_size=reduced_hidden_size,\n",
        "    num_hidden_layers=reduced_num_hidden_layers,\n",
        "    num_attention_heads=reduced_num_attention_heads,\n",
        ")"
      ],
      "metadata": {
        "id": "RSHWR9c-hDSA"
      },
      "id": "RSHWR9c-hDSA",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFAlbertModel.from_pretrained(model_name, config=new_config, ignore_mismatched_sizes=True)\n",
        "tokenizer = AlbertTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMotZs_ghGBw",
        "outputId": "00b3aca5-6858-4161-d4f2-189d8c1d5059"
      },
      "id": "XMotZs_ghGBw",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['albert.encoder.embedding_hidden_mapping_in.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias', 'albert.encoder.embedding_hidden_mapping_in.weight', 'predictions.LayerNorm.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight', 'predictions.LayerNorm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight', 'predictions.decoder.bias', 'albert.pooler.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight', 'albert.pooler.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias']\n",
            "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
            "Some weights of TFAlbertModel were not initialized from the model checkpoint are newly initialized because the shapes did not match:\n",
            "- albert.encoder.embedding_hidden_mapping_in.weight: found shape (768, 128) in the checkpoint and (128, 12) in the model instantiated\n",
            "- albert.encoder.embedding_hidden_mapping_in.bias: found shape (768,) in the checkpoint and (12,) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight: found shape (768, 768) in the checkpoint and (12, 12) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias: found shape (768,) in the checkpoint and (12,) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight: found shape (768, 768) in the checkpoint and (12, 12) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias: found shape (768,) in the checkpoint and (12,) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight: found shape (768, 768) in the checkpoint and (12, 12) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias: found shape (768,) in the checkpoint and (12,) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight: found shape (768, 768) in the checkpoint and (12, 12) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias: found shape (768,) in the checkpoint and (12,) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight: found shape (768,) in the checkpoint and (12,) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias: found shape (768,) in the checkpoint and (12,) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight: found shape (3072, 768) in the checkpoint and (12, 16384) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias: found shape (3072,) in the checkpoint and (16384,) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight: found shape (768, 3072) in the checkpoint and (16384, 12) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias: found shape (768,) in the checkpoint and (12,) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight: found shape (768,) in the checkpoint and (12,) in the model instantiated\n",
            "- albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias: found shape (768,) in the checkpoint and (12,) in the model instantiated\n",
            "- albert.pooler.weight: found shape (768, 768) in the checkpoint and (12, 12) in the model instantiated\n",
            "- albert.pooler.bias: found shape (768,) in the checkpoint and (12,) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "63833eae",
      "metadata": {
        "id": "63833eae"
      },
      "outputs": [],
      "source": [
        "def embed_words(words):\n",
        "    tokens = tokenizer.batch_encode_plus(words, padding='max_length', truncation=True, return_tensors='tf', max_length=64)\n",
        "    outputs = model(tokens['input_ids'])\n",
        "    embeddings = outputs.last_hidden_state\n",
        "    averaged_embeddings = tf.reduce_mean(embeddings, axis=1)\n",
        "    return averaged_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = r\"/content/drive/MyDrive/Reuters-21578/reuters/reuters/reuters/training\"\n",
        "dataset_dirs = os.listdir(dataset_path)"
      ],
      "metadata": {
        "id": "zvK2TyXqY3bk"
      },
      "id": "zvK2TyXqY3bk",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for i in tqdm(dataset_dirs):\n",
        "    with open(f\"{dataset_path}/{i}\", 'r') as f:\n",
        "        content = f.read()\n",
        "        data.append(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y054Hz54Y9bJ",
        "outputId": "4e191296-73ea-46ab-faae-05d8cd6d5fcb"
      },
      "id": "Y054Hz54Y9bJ",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7769/7769 [00:30<00:00, 256.92it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "for text in data:\n",
        "  temp_words = word_tokenize(text)\n",
        "  words.append(temp_words)"
      ],
      "metadata": {
        "id": "yHlmyhIy3-uI"
      },
      "id": "yHlmyhIy3-uI",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "words_flattened = list(chain(*words))\n"
      ],
      "metadata": {
        "id": "CzSR7XPHaVUB"
      },
      "id": "CzSR7XPHaVUB",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words_flattened)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkUueqI_hd6z",
        "outputId": "d4430c37-f1fe-445f-a49f-d491170e8f9e"
      },
      "id": "LkUueqI_hd6z",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1135633"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_flattened = [str(element) for element in words_flattened]"
      ],
      "metadata": {
        "id": "f78itPlWjBr6"
      },
      "id": "f78itPlWjBr6",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48f40c52",
      "metadata": {
        "id": "48f40c52"
      },
      "outputs": [],
      "source": [
        "embeddings = embed_words(words_flattened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca00c34f",
      "metadata": {
        "id": "ca00c34f"
      },
      "outputs": [],
      "source": [
        "# Save embeddings and metadata in TSV format\n",
        "tsv_file = 'word_embeddings.tsv'\n",
        "metadata_file = 'metadata.tsv'\n",
        "\n",
        "# Save embeddings\n",
        "np.savetxt(tsv_file, embeddings, delimiter='\\t')\n",
        "\n",
        "# Save metadata\n",
        "with open(metadata_file, 'w', encoding='utf-8', newline='') as file:\n",
        "    writer = csv.writer(file, delimiter='\\t')\n",
        "    writer.writerows([[word] for word in words_flattened])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36a8031e",
      "metadata": {
        "id": "36a8031e"
      },
      "outputs": [],
      "source": [
        "len(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55b0be06",
      "metadata": {
        "id": "55b0be06"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}